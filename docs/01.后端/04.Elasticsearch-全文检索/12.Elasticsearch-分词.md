---
title: Elasticsearch-分词
date: 2020-11-25 14:52:20
permalink: /pages/c68b46/
categories:
  - 后端
  - Elasticsearch-全文检索
tags:
  - Elasticsearch
  - 分词
  - tokenizer
---

## 前言
我们前面学习Elasticsearch一直说分词，好像很熟悉，但是未作深入了解，分词到底是什么？下面来学习。

## 分词
分词是Elasticsearch全文检索的核心，分词是将一大段完整的句子，分成一个个单词，然后利用单词进行相关性匹配，最终完成全文检索的功能。

## 分词器（tokenizer）
在ES中，是用分词器(`tokenizer`)来进行分词，一个`tokenizer`（分词器）接收一个字符流，将之分割为独立的`tokens`（词元，通常是独立的单词），然后输出`tokens`流。

例如：`whitesapce tokenizer`遇到空白字符时分割文本。它会将文本“`Quick brown fox!`”分割为[`Qucik`,`brown`,`fox!`]。

​		该`tokenizer`（分词器）还负责记录各个`term`(词条)的顺序或者`position`（位置）（用于`phrase`短语和`word proximity`词近邻查询）,以及`term`（词条）所代表的的原始`word`（单词）的`start`和`end`的`chaaracter offsets`（字符偏移量，用于高亮显示索索的内容）。

​		`Elasticsearch`提供了很多内置的分词器，可以用来构建`custom analyzers`（自定义分词器）。

​		`Elasticsearch`默认使用的分词器是[标准分词器](https://www.elastic.co/guide/en/elasticsearch/reference/7.5/analysis-standard-analyzer.html)。



## 标准分词器

> The `standard` analyzer is the default analyzer which is used if none is specified. It provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in [Unicode Standard Annex #29](http://unicode.org/reports/tr29/)) and works well for most languages.

### 官方例子

~~~json
# 分词请求
POST _analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
# 返回
[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]
~~~

### 例子2

~~~json
# 分词请求
POST _analyze
{
  "analyzer": "standard",
  "text": "我是中国人，我想吃重庆火锅！"
}
# 返回
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "中",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    },
    {
      "token" : "国",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "人",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "<IDEOGRAPHIC>",
      "position" : 4
    },
    {
      "token" : "我",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "<IDEOGRAPHIC>",
      "position" : 5
    },
    {
      "token" : "想",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "<IDEOGRAPHIC>",
      "position" : 6
    },
    {
      "token" : "吃",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<IDEOGRAPHIC>",
      "position" : 7
    },
    {
      "token" : "重",
      "start_offset" : 9,
      "end_offset" : 10,
      "type" : "<IDEOGRAPHIC>",
      "position" : 8
    },
    {
      "token" : "庆",
      "start_offset" : 10,
      "end_offset" : 11,
      "type" : "<IDEOGRAPHIC>",
      "position" : 9
    },
    {
      "token" : "火",
      "start_offset" : 11,
      "end_offset" : 12,
      "type" : "<IDEOGRAPHIC>",
      "position" : 10
    },
    {
      "token" : "锅",
      "start_offset" : 12,
      "end_offset" : 13,
      "type" : "<IDEOGRAPHIC>",
      "position" : 11
    }
  ]
}
~~~

发现它居然把每个汉字都分成一个次，显然不符合我们的预想，按我的想法肯定会把**中国人**、**火锅**放在一个词里面。

显然标准分词器只能满足英文的分词，中文的分词还需要安装一个开源的`ik分词器`。



## 安装ik分词器

[github地址](https://github.com/medcl/elasticsearch-analysis-ik)

### 安装步骤

1. 进入es容器内部的`plugins`目录
2. 下载跟当前es同版本的分词器
3. 解压

由于以前我们已经把es挂载到外部的`/mydata/elasticearch/pugins`目录，直接安装在这个目录下就可以了。