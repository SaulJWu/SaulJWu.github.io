---
title: Elasticsearch-分词
date: 2020-11-25 14:52:20
permalink: /pages/c68b46/
categories:
  - 后端
  - Elasticsearch-全文检索
tags:
  - Elasticsearch
  - 分词
  - tokenizer
---

## 前言
我们前面学习Elasticsearch一直说分词，好像很熟悉，但是未作深入了解，分词到底是什么？下面来学习。

## 分词
分词是Elasticsearch全文检索的核心，分词是将一大段完整的句子，分成一个个单词，然后利用单词进行相关性匹配，最终完成全文检索的功能。

## 分词器（tokenizer）
在ES中，是用分词器(`tokenizer`)来进行分词，一个`tokenizer`（分词器）接收一个字符流，将之分割为独立的`tokens`（词元，通常是独立的单词），然后输出`tokens`流。

例如：`whitesapce tokenizer`遇到空白字符时分割文本。它会将文本“`Quick brown fox!`”分割为[`Qucik`,`brown`,`fox!`]。

​		该`tokenizer`（分词器）还负责记录各个`term`(词条)的顺序或者`position`（位置）（用于`phrase`短语和`word proximity`词近邻查询）,以及`term`（词条）所代表的的原始`word`（单词）的`start`和`end`的`chaaracter offsets`（字符偏移量，用于高亮显示索索的内容）。

​		`Elasticsearch`提供了很多内置的分词器，可以用来构建`custom analyzers`（自定义分词器）。

​		`Elasticsearch`默认使用的分词器是[标准分词器](https://www.elastic.co/guide/en/elasticsearch/reference/7.5/analysis-standard-analyzer.html)。



## 标准分词器

> The `standard` analyzer is the default analyzer which is used if none is specified. It provides grammar based tokenization (based on the Unicode Text Segmentation algorithm, as specified in [Unicode Standard Annex #29](http://unicode.org/reports/tr29/)) and works well for most languages.

### 官方例子

~~~json
# 分词请求
POST _analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
# 返回
[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]
~~~

### 例子2

~~~json
# 分词请求
POST _analyze
{
  "analyzer": "standard",
  "text": "我是中国人，我想吃重庆火锅！"
}
# 返回
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "<IDEOGRAPHIC>",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "<IDEOGRAPHIC>",
      "position" : 1
    },
    {
      "token" : "中",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "<IDEOGRAPHIC>",
      "position" : 2
    },
    {
      "token" : "国",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "<IDEOGRAPHIC>",
      "position" : 3
    },
    {
      "token" : "人",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "<IDEOGRAPHIC>",
      "position" : 4
    },
    {
      "token" : "我",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "<IDEOGRAPHIC>",
      "position" : 5
    },
    {
      "token" : "想",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "<IDEOGRAPHIC>",
      "position" : 6
    },
    {
      "token" : "吃",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<IDEOGRAPHIC>",
      "position" : 7
    },
    {
      "token" : "重",
      "start_offset" : 9,
      "end_offset" : 10,
      "type" : "<IDEOGRAPHIC>",
      "position" : 8
    },
    {
      "token" : "庆",
      "start_offset" : 10,
      "end_offset" : 11,
      "type" : "<IDEOGRAPHIC>",
      "position" : 9
    },
    {
      "token" : "火",
      "start_offset" : 11,
      "end_offset" : 12,
      "type" : "<IDEOGRAPHIC>",
      "position" : 10
    },
    {
      "token" : "锅",
      "start_offset" : 12,
      "end_offset" : 13,
      "type" : "<IDEOGRAPHIC>",
      "position" : 11
    }
  ]
}
~~~

发现它居然把每个汉字都分成一个次，显然不符合我们的预想，按我的想法肯定会把**中国人**、**火锅**放在一个词里面。

显然标准分词器只能满足英文的分词，中文的分词还需要安装一个开源的`ik分词器`。





## 安装ik分词器

[github地址](https://github.com/medcl/elasticsearch-analysis-ik)

### 安装步骤

- 进入es容器内部的`plugins`目录

~~~sh
#b7e是容器id前三位
docker exec -it b7e /bin/bash

# 进入到目录
cd plugins/

# 创建一个专用目录
mkdir ik

# 进入到专用目录
cd ik/

# 下载跟当前es同版本的分词器如果没安装`wget`，输入`yum install wget`回车安装
wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip

# 解压 输入前几位就可以自动补全，如果没有安装`unzip`，输入`yum install unzip`
unzip #下载的文件

# 查看文件夹权限，发现是文件夹蓝色
ll

# 更改ik文件夹权限
chmod -R 777 ik/

# 再次查看文件夹权限，已经变成绿色 drwxrwxrwx
ll

# 进入docker内容内部
docker exec -it b7e /bin/bash

# 查看容器内部挂载，到底同步了没
cd plugins/

# 查看文件夹,发现已经安装上
ll

# 回到上一级目录
cd ../

# 进入bin目录
cd bin/

# 查看bin下的文件，有一个elasticsearch-plugin，可以执行安装插件
ls

# 执行插件
elasticsearch-plugin

# 会提示命令
elasticsearch-plugin -h

# 根据提示命令，查看一下已经安装的插件，回显ik，说明已经安装好了
elasticsearch-plugin list

# 退出容器
exit

# 重启容器
docker restart ee1 #不写容器id也可以写elasticsearch

~~~



## 测试分词

> The IK Analysis plugin integrates Lucene IK analyzer (http://code.google.com/p/ik-analyzer/) into elasticsearch, support customized dictionary.
>
> Analyzer: `ik_smart` , `ik_max_word` , Tokenizer: `ik_smart` , `ik_max_word`

通过[github官方](https://github.com/medcl/elasticsearch-analysis-ik)，我们来测试一下刚才“我是中国人，我想吃重庆火锅”的分词有什么区别

### ik_smart

~~~json
# 请求
POST _analyze
{
  "analyzer": "ik_smart",
  "text": "我是中国人，我想吃重庆火锅！"
}
# 返回
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "中国人",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "我",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "CN_CHAR",
      "position" : 3
    },
    {
      "token" : "想吃",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "CN_WORD",
      "position" : 4
    },
    {
      "token" : "重庆火锅",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "CN_WORD",
      "position" : 5
    }
  ]
}
~~~

### ik_max_word

~~~json
# 请求
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "我是中国人，我想吃重庆火锅！"
}
# 返回
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "是",
      "start_offset" : 1,
      "end_offset" : 2,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "中国人",
      "start_offset" : 2,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "中国",
      "start_offset" : 2,
      "end_offset" : 4,
      "type" : "CN_WORD",
      "position" : 3
    },
    {
      "token" : "国人",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 4
    },
    {
      "token" : "我",
      "start_offset" : 6,
      "end_offset" : 7,
      "type" : "CN_CHAR",
      "position" : 5
    },
    {
      "token" : "想吃",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "CN_WORD",
      "position" : 6
    },
    {
      "token" : "吃重",
      "start_offset" : 8,
      "end_offset" : 10,
      "type" : "CN_WORD",
      "position" : 7
    },
    {
      "token" : "重庆火锅",
      "start_offset" : 9,
      "end_offset" : 13,
      "type" : "CN_WORD",
      "position" : 8
    },
    {
      "token" : "重庆",
      "start_offset" : 9,
      "end_offset" : 11,
      "type" : "CN_WORD",
      "position" : 9
    },
    {
      "token" : "火锅",
      "start_offset" : 11,
      "end_offset" : 13,
      "type" : "CN_WORD",
      "position" : 10
    }
  ]
}
~~~

这就是ik分词器分词的效果。但是似乎有些不如人意。

比如：

~~~json
# 请求
POST _analyze
{
  "analyzer": "ik_max_word",
  "text": "淘宝电商项目"
}
# 返回
{
  "tokens" : [
    {
      "token" : "淘宝",
      "start_offset" : 0,
      "end_offset" : 2,
      "type" : "CN_WORD",
      "position" : 0
    },
    {
      "token" : "电",
      "start_offset" : 2,
      "end_offset" : 3,
      "type" : "CN_CHAR",
      "position" : 1
    },
    {
      "token" : "商",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "CN_CHAR",
      "position" : 2
    },
    {
      "token" : "项目",
      "start_offset" : 4,
      "end_offset" : 6,
      "type" : "CN_WORD",
      "position" : 3
    }
  ]
}
~~~

**电商**这个词没有出现，没有达到我预想的结果，不过没关系，接下来我们来自定义分词规则。



## 自定义分词